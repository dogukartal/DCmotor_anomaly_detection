{
  "experiment_id": "hpo",
  "description": "Hyperparameter optimization configuration for LSTM autoencoder",

  "base_configs": {
    "simulation": "configs/simulation/default.json",
    "model": "configs/model/default.json"
  },

  "hyperparameter_optimization": {
    "enabled": true,
    "n_trials": 50,
    "metric": "val_loss",
    "direction": "minimize",
    "parameters": {
      "model.encoder_units": {
        "type": "categorical",
        "choices": [[32], [64], [64, 32], [128, 64]]
      },
      "model.bottleneck.units": {
        "type": "int",
        "low": 8,
        "high": 64,
        "step": 8,
        "constraint": {
          "type": "max_from_last",
          "parameter": "model.encoder_units",
          "multiplier": 1.0
        }
      },
      "training.epochs": {
        "type": "int",
        "low": 30,
        "high": 150,
        "step": 10
      },
      "training.optimizer.learning_rate": {
        "type": "loguniform",
        "low": 1e-5,
        "high": 1e-2
      },
      "training.batch_size": {
        "type": "categorical",
        "choices": [16, 32, 64, 128]
      },
      "training.lr_scheduler.patience": {
        "type": "int",
        "low": 3,
        "high": 15,
        "step": 1
      },
      "physics_loss.weight": {
        "type": "uniform",
        "low": 0.01,
        "high": 1.0
      },
      "physics_loss.start_epoch": {
        "type": "int",
        "low": 0,
        "high": 30,
        "step": 5
      }
    }
  }
}
